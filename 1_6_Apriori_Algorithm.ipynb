{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_6 Apriori Algorithm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6krBWyeFlEtFu97dL+ig2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skssushil/Web-mining/blob/master/1_6_Apriori_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omjMnIj9wzq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2f479f6d-8103-4456-83a9-3d1f5581271e"
      },
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import re\n",
        "nltk.download('punkt');\n",
        "nltk.download('stopwords');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F6csa1AyB_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read in data\n",
        "corpus = []\n",
        "dataSetFilename = 'text1.txt'\n",
        "with open(dataSetFilename,'r') as file:\n",
        "  corpus=file.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQjx8qKrxRsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N=len(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAO5-5Efy0Hu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3a5c5d7-50df-4867-80cc-e83c550d717d"
      },
      "source": [
        "N"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27715"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmo3i_7h0DSc",
        "colab_type": "text"
      },
      "source": [
        "##Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QFmPLz80FQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_lower_case(data):\n",
        "  return np.char.lower(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3RWg1lg0JXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stop_words(data):\n",
        "  stop_words = stopwords.words('english')\n",
        "  words = word_tokenize(str(data))\n",
        "  new_text = \"\"\n",
        "  for w in words:\n",
        "    if w not in stop_words and len(w) > 1:\n",
        "      new_text = new_text + \" \" + w\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2Qy3kgS0LLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(data):\n",
        "  symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "  for i in range(len(symbols)):\n",
        "    data = np.char.replace(data, symbols[i], ' ')\n",
        "    data = np.char.replace(data, \"  \", \" \")\n",
        "    data = np.char.replace(data, ',', '')\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7LYQoNL0NI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_apostrophe(data):\n",
        "  return np.char.replace(data, \"'\", \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt1xZKHY0QLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemming(data):\n",
        "  stemmer= PorterStemmer()  \n",
        "  tokens = word_tokenize(str(data))\n",
        "  new_text = \"\"\n",
        "  for w in tokens:\n",
        "    new_text = new_text + \" \" + stemmer.stem(w)\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSCT9pwG0SLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import inflect \n",
        "p = inflect.engine()   \n",
        "# convert number into words \n",
        "def convert_number(text): \n",
        "  temp_str = text.split() \n",
        "  new_string = []  \n",
        "  for word in temp_str:\n",
        "    if word.isdigit(): \n",
        "      temp = p.number_to_words(word) \n",
        "      new_string.append(temp) \n",
        "    else:\n",
        "      new_string.append(word) \n",
        "      temp_str = ' '.join(new_string) \n",
        "  return temp_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbIHw-FD0V70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(data):\n",
        "  data = convert_number(data)\n",
        "  data = convert_lower_case(data)\n",
        "  data = remove_punctuation(data) #remove comma seperately\n",
        "  data = remove_apostrophe(data)\n",
        "  data = remove_stop_words(data)\n",
        "  data = stemming(data)\n",
        "  data = remove_punctuation(data)\n",
        "  data = stemming(data) #needed again as we need to stem the words\n",
        "  data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
        "  data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
        "  data = convert_number(data)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFFlBZ7O0YNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens=word_tokenize(preprocess(corpus))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on67CsQkrYLq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "58ba601c-1d8f-4908-9473-e337e50ae628"
      },
      "source": [
        "print(\" Vocabs Length \"\"\\n\",len(tokens))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Vocabs Length \n",
            " 2381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STMns-mXd6d8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "bbde5b22-7829-4053-d56a-c678df15c263"
      },
      "source": [
        "print(\" Vocabs \"\"\\n\",*tokens[:500])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Vocabs \n",
            " emma woodhou handsom clever rich comfort home happi disposit seem unit best bless exist live nearli twenti one year world littl distress vex youngest two daughter affect indulg father consequ sister marriag mistress hou earli period mother die long ago indistinct remembr caress place suppli excel woman gover fallen littl short mother affect sixteen year miss taylor mr woodhou famili less gover friend fond daughter particularli emma intimaci sister even miss taylor cea hold nomin offic gover mild temper hardli allow impo restraint shadow author long pass away live togeth friend friend mutual attach emma like highli esteem miss taylor judgment direct chiefli real evil ind emma situat power rather much way disposit think littl well disadvantag threaten alloy mani enjoy danger howev present unperceiv mean rank misfortun sorrow came—a gentl sorrow—but shape disagr consciou —miss taylor marri miss taylor loss first brought grief wed day belov friend emma first sat mourn thought continu wed bride peopl gone father left dine togeth prospect third cheer long even father compo sleep dinner usual sit think lost event everi promi happi friend mr weston man unexcept charact easi fortun suitabl age pleasant manner satisfact consid self deni gener friendship alway wish promot match black morn work want miss taylor would felt everi hour everi day recal past kindness—th kind affect sixteen years—how taught play five year old—how devot power attach amu health—and nur variou ill childhood larg debt gratitud owe intercour last seven year equal foot perfect unreserv soon follow isabella marriag left yet dearer tender recollect friend companion possess intellig well inform use gentl know way famili interest concern peculiarli interest everi pleasur everi scheme hers—on could speak everi thought aro affect could never find fault bear chang —it true friend go half mile emma awar great must differ mr weston half mile miss taylor hou advantag natur domest great danger suffer intellectu solitud dearli love father companion could meet conver ration play evil actual dispar age mr woodhou marri earli much increa constitut habit valetudinarian life without activ mind bodi much older man way year though everywh belov friendli heart amiabl temper talent could recommend time sister though compar littl remov matrimoni settl london sixteen mile much beyond daili reach mani long octob novemb even must struggl hartfield christma brought next visit isabella husband littl children fill hou give pleasant societi highburi larg popul villag almost amount town hartfield spite separ lawn shrubberi name realli belong afford equal woodhou first consequ look mani acquaint place father univ civil one among could accept lieu miss taylor even half day melancholi chang emma could sigh wish imposs thing till father awok made necessari cheer spirit requir support nervou man easili depress fond everi bodi use hate part hate chang everi kind matrimoni origin chang alway disagr mean yet reconcil daughter marri could ever speak compass though entir match affect oblig part miss taylor habit gentl selfish never abl suppo peopl could feel differ much dispo think miss taylor done sad thing would\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyq89GrQPJV8",
        "colab_type": "text"
      },
      "source": [
        "##Apriori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNbTIqw48Zb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createCandidateSet(data):\n",
        "\tcand = []\n",
        "\tfor row in data:\n",
        "\t\tfor itm in row:\n",
        "\t\t\tif [itm] not in cand:\n",
        "\t\t\t\tcand.append([itm])\n",
        "\tcand.sort()\n",
        "\treturn list(map(frozenset,cand))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV-_B6-W8Z8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scanData(data, candidateSet, minSupport):\n",
        "\tsubsetCount = {}\n",
        "\tfor curSet in data:\n",
        "\t\tfor cand in candidateSet:\n",
        "\t\t\tif cand.issubset(curSet):\n",
        "\t\t\t\tif not cand in subsetCount:\n",
        "\t\t\t\t\tsubsetCount[cand] = 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tsubsetCount[cand] += 1\n",
        "\tn = float(len(data))\n",
        "\tvalid = []\n",
        "\tfor key in subsetCount:\n",
        "\t\tsup = subsetCount[key]\n",
        "\t\tif sup >= minSupport:\n",
        "\t\t\tvalid.insert(0,key)\n",
        "\treturn valid, subsetCount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imIpafAe8eTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def genApriori(freqSets, k):\n",
        "\tvalid = []\n",
        "\tnFreqSets = len(freqSets)\n",
        "\tfor i in range(nFreqSets):\n",
        "\t\tfor j in range(i+1, nFreqSets):\n",
        "\t\t\tlstCands1 = list(freqSets[i])[:k-2]\n",
        "\t\t\tlstCands2 = list(freqSets[j])[:k-2]\n",
        "\t\t\tlstCands1.sort()\n",
        "\t\t\tlstCands2.sort()\n",
        "\t\t\t# if first k-2 elements are equal\n",
        "\t\t\tif lstCands1 == lstCands2:\n",
        "\t\t\t\tvalid.append(freqSets[i]|freqSets[j]) # union \n",
        "\treturn valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wewl35tN8gbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apriori(data, minSupport):\n",
        "\tcandSet = createCandidateSet(data)\n",
        "\tsetData = list(map(set,data))\n",
        "\tlstCands, subsetCounts = scanData(setData,candSet,minSupport)\n",
        "\tlstCands = [lstCands]\n",
        "\tk = 2\n",
        "\twhile(len(lstCands[k-2]) > 0):\n",
        "\t\tcandSetX = genApriori(lstCands[k-2],k)\n",
        "\t\tlstCandsX, subsetCountsX = scanData(setData,candSetX, minSupport)\n",
        "\t\tsubsetCounts.update(subsetCountsX)\n",
        "\t\tlstCands.append(candSetX)\n",
        "\t\tk += 1\n",
        "\treturn lstCands, subsetCounts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-N6K6Or8iky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read in data\n",
        "data = []\n",
        "dataSetFilename = 'text.txt'\n",
        "with open(dataSetFilename,'r') as file:\n",
        "\tfor line in file:\n",
        "\t\tdata.append(line.strip().split(','))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq0z624f8kov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fb52a5b9-1ff0-4241-d5c9-3d7c9471ccf0"
      },
      "source": [
        "print(\"What min. support do you want to use? \")\n",
        "minSupp = input()\n",
        "minSupp = int(minSupp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What min. support do you want to use? \n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN2Y1xyj8nMk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "7577b205-63cb-4147-982a-58279ed68c25"
      },
      "source": [
        "print(\"\\n**** Apriori with minSupport = {} ****\".format(minSupp))\n",
        "# call apriori\n",
        "sets, counts = apriori(data,minSupp)\n",
        "print(\"\\nSets:\")\n",
        "\"\"\"for x in sets:\n",
        "\tfor y in x:\n",
        "\t\tprint(y)\"\"\"\n",
        "print(\"\\n Most Frequent items of size upto 5: \\n\")\n",
        "for k,v in sorted(counts.items(), reverse=True, key=lambda tup: tup[1])[:20]:\n",
        "\tprint(k, v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "**** Apriori with minSupport = 2 ****\n",
            "\n",
            "Sets:\n",
            "\n",
            " Most Frequent items of size upto 5: \n",
            "\n",
            "frozenset({'BabyRuth'}) 49\n",
            "frozenset({'Snickers'}) 47\n",
            "frozenset({'Hershey'}) 43\n",
            "frozenset({'AlmondJoy'}) 41\n",
            "frozenset({'KitKat'}) 40\n",
            "frozenset({'Snickers', 'Hershey'}) 40\n",
            "frozenset({'BabyRuth', 'Snickers'}) 40\n",
            "frozenset({'BabyRuth', 'Hershey'}) 39\n",
            "frozenset({'BabyRuth', 'Snickers', 'Hershey'}) 36\n",
            "frozenset({'HeathBar'}) 35\n",
            "frozenset({'Twix'}) 35\n",
            "frozenset({'MilkyWay'}) 34\n",
            "frozenset({'Snickers', 'KitKat'}) 34\n",
            "frozenset({'Cotton Candy'}) 33\n",
            "frozenset({'BabyRuth', 'KitKat'}) 33\n",
            "frozenset({'Cadbury'}) 32\n",
            "frozenset({'Snickers', 'AlmondJoy'}) 32\n",
            "frozenset({'BabyRuth', 'AlmondJoy'}) 31\n",
            "frozenset({'Hershey', 'KitKat'}) 30\n",
            "frozenset({'AlmondJoy', 'Hershey'}) 29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brK_tZu88qsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}